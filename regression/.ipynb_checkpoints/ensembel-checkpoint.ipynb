{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import time\n",
    "import math\n",
    "import xgboost\n",
    "\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "timeformat = \"%Y-%m-%d %H:%M:%S\"\n",
    "locations = [\"EL\", \"LB\", \"NT\", \"CH\"]\n",
    "\n",
    "data = pd.read_csv(\"../data/ensemble.csv\")\n",
    "date = pd.read_csv(\"../data/date.csv\")\n",
    "\n",
    "data = pd.concat([data,date], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_angle(angle):\n",
    "    return (angle*math.pi)/180.0\n",
    "\n",
    "def wind(cell, angle):\n",
    "    cell = (cell - angle)\n",
    "    weight = math.cos(to_angle(cell))\n",
    "    # ReLU\n",
    "    return weight * (weight > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    data['EL_PM25_' + str(i+1)] = data['EL_PM25'].shift(i+1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for i in range(3):\n",
    "    data['LB_PM25_' + str(i+1)] = data['LB_PM25'].shift(i+1)\n",
    "    data['CH_PM25_' + str(i+1)] = data['CH_PM25'].shift(i+1)\n",
    "    data['NT_PM25_' + str(i+1)] = data['NT_PM25'].shift(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 風向的權重\n",
    "data['LB_WD_W'] = data.LB_WD.apply(wind, args=(198.39,))\n",
    "data['CH_WD_W'] = data.CH_WD.apply(wind, args=(40.51,))\n",
    "data['NT_WD_W'] = data.NT_WD.apply(wind, args=(92.71,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 與二林PM25的差值\n",
    "data['LB_D_PM25'] = data.LB_PM25 - data.EL_PM25\n",
    "data['NT_D_PM25'] = data.NT_PM25 - data.EL_PM25\n",
    "data['CH_D_PM25'] = data.CH_PM25 - data.EL_PM25"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 得分 = 差值 * 權重\n",
    "data['LB_S'] = data.LB_D_PM25 * data.LB_WD_W\n",
    "data['CH_S'] = data.CH_D_PM25 * data.CH_WD_W\n",
    "data['NT_S'] = data.NT_D_PM25 * data.NT_WD_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 目標值\n",
    "data['target'] = data.EL_PM25.shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# sklearn minmax scaler\n",
    "X=pd.DataFrame([1,2,3,4,5])\n",
    "X = X.dropna()\n",
    "X = X.values.reshape(len(X), 1)\n",
    "print(X)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = scaler.fit(X)\n",
    "scaler_X = scaler.transform(X)\n",
    "scaled_series = pd.DataFrame(scaler_X)\n",
    "print(scaled_series.head())\n",
    "# invert transform\n",
    "inverted_X = scaler.inverse_transform(scaler_X)\n",
    "inverted_series = pd.DataFrame(inverted_X[:, 0])\n",
    "print(inverted_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PM25 = [\"EL_PM25\"]\n",
    "hour_cols = [f for f in data.columns.tolist() if \"h_\" in f]\n",
    "month_cols = [f for f in data.columns.tolist() if \"m_\" in f]\n",
    "exclude_cols = ['datetime', 'target']\n",
    "meteorology_cols = ['EL_TEMP', 'EL_RH', 'EL_RAINFALL', 'EL_WS']\n",
    "lags_cols = [f for f in data.columns.tolist() if 'EL_PM25_' in f]\n",
    "wd_cols = [\"LB_WD_W\", \"CH_WD_W\", \"NT_WD_W\"]\n",
    "neighbors_cols = [\"LB_PM25\", \"CH_PM25\", \"NT_PM25\"]\n",
    "neighbors_D_cols = [\"LB_D_PM25\", \"CH_D_PM25\", \"NT_D_PM25\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Season Clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "...\n",
    "winter = data[(data.month_12 == 1) | (data.month_1 == 1) | (data.month_2 == 1)]\n",
    "spring = data[(data.month_3 == 1) | (data.month_4 == 1) | (data.month_5 == 1)]\n",
    "summer = data[(data.month_6 == 1) | (data.month_7 == 1) | (data.month_8 == 1)]\n",
    "fall = data[(data.month_9 == 1) | (data.month_10 == 1) | (data.month_11 == 1)]\n",
    "\n",
    "season = [winter, spring, summer, fall]\n",
    "\n",
    "for each_season in season:\n",
    "    train = each_season[each_season.datetime.values < '2016-01-01 00:00:00'].dropna()\n",
    "    test = each_season[each_season.datetime.values >= '2016-01-01 00:00:00'].dropna()\n",
    "    print train.shape, test.shape\n",
    "    \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = data[data.datetime.values == '2016-01-01 00:00:00'].index[0]\n",
    "\n",
    "datatrain = data[:start]\n",
    "datatest = data[start:]\n",
    "\n",
    "column = PM25 + exclude_cols + meteorology_cols + lags_cols + hour_cols + wd_cols + neighbors_D_cols\n",
    "\n",
    "train = datatrain[column].dropna()\n",
    "test = datatest[column].dropna()\n",
    "\n",
    "A_train, A_test = train[PM25 + hour_cols + lags_cols], test[PM25 + hour_cols + lags_cols]\n",
    "y_train, y_test = train[\"target\"], test[\"target\"]\n",
    "B_train, B_test = train[PM25 + neighbors_D_cols], test[PM25 + neighbors_D_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Model:\n",
      "EL_PM25, h_0, h_1, h_2, h_3, h_4, h_5, h_6, h_7, h_8, h_9, h_10, h_11, h_12, h_13, h_14, h_15, h_16, h_17, h_18, h_19, h_20, h_21, h_22, h_23, EL_PM25_1, EL_PM25_2, EL_PM25_3, EL_PM25_4, EL_PM25_5, EL_PM25_6\n"
     ]
    }
   ],
   "source": [
    "print(\"A Model:\\n\" + \", \".join(A_train.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B Model:\n",
      "EL_PM25, LB_D_PM25, CH_D_PM25, NT_D_PM25\n"
     ]
    }
   ],
   "source": [
    "print(\"B Model:\\n\" + \", \".join(B_train.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consume: 0.100032 s.\n",
      "L rmse:  4.719499\n"
     ]
    }
   ],
   "source": [
    "# Linear Model\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "start_time = time.time()\n",
    "L = LinearRegression()\n",
    "L.fit(A_train, y_train)\n",
    "L_list = L.predict(A_test)\n",
    "L_rmse = sqrt(mean_squared_error(L_list, y_test))\n",
    "end_time = time.time()\n",
    "print(\"Time consume: %f s.\"%(end_time - start_time))\n",
    "print(\"L rmse: \", round(L_rmse, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P rmse:  5.511364\n"
     ]
    }
   ],
   "source": [
    "# Persistence Model\n",
    "P_list=A_test[\"EL_PM25\"]\n",
    "P_rmse = sqrt(mean_squared_error(P_list, y_test))\n",
    "print(\"P rmse: \", round(P_rmse, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consume: 5.965839 s.\n",
      "A rmse: 4.543035\n",
      "B rmse: 5.413168\n",
      "C rmse: 4.558493\n"
     ]
    }
   ],
   "source": [
    "rmse_list_A =list()\n",
    "rmse_list_B =list()\n",
    "rmse_list_C =list()\n",
    "t = time.time()\n",
    "for _ in range(1):\n",
    "    # A model\n",
    "    A = neural_network.MLPRegressor()\n",
    "    A.fit(A_train, y_train)\n",
    "    A_preds = A.predict(A_test)\n",
    "    A_rmse = sqrt(mean_squared_error(A_preds, y_test))\n",
    "    rmse_list_A.append(A_rmse)\n",
    "    \n",
    "    # B model\n",
    "    B = LinearRegression()\n",
    "    B.fit(B_train, y_train)\n",
    "    B_preds = B.predict(B_test)\n",
    "    B_rmse = sqrt(mean_squared_error(B_preds, y_test))\n",
    "    rmse_list_B.append(B_rmse)\n",
    "    \n",
    "    # C test data\n",
    "    C_test = pd.DataFrame({'A_preds':A_preds, 'B_preds':B_preds})\n",
    "    # C train data\n",
    "    A_predict = A.predict(A_train)\n",
    "    B_predict = B.predict(B_train)\n",
    "    C_train = pd.DataFrame({'A_preds':A_predict, 'B_preds':B_predict})\n",
    "    C_train_Feature = train[PM25 + meteorology_cols + wd_cols].reset_index(drop=True)\n",
    "    C_test_Feature = test[PM25 + meteorology_cols + wd_cols].reset_index(drop=True)\n",
    "    C_train = pd.concat([C_train, C_train_Feature], axis=1)\n",
    "    C_test = pd.concat([C_test, C_test_Feature], axis=1)\n",
    "\n",
    "    # C model\n",
    "    C = xgboost.XGBRegressor()\n",
    "    C.fit(C_train, y_train)\n",
    "    C_list = C.predict(C_test)\n",
    "    C_rmse = sqrt(mean_squared_error(C_list, y_test))\n",
    "    rmse_list_C.append(C_rmse)\n",
    "    \n",
    "print(\"Time consume: %f s.\"%(time.time() - t))\n",
    "print(\"A rmse: %f\"%(sum(rmse_list_A)/float(len(rmse_list_A))))\n",
    "print(\"B rmse: %f\"%(sum(rmse_list_B)/float(len(rmse_list_B))))\n",
    "print(\"C rmse: %f\"%(sum(rmse_list_C)/float(len(rmse_list_C))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Model:\n",
      "A_preds, B_preds, EL_PM25, EL_TEMP, EL_RH, EL_RAINFALL, EL_WS, LB_WD_W, CH_WD_W, NT_WD_W\n"
     ]
    }
   ],
   "source": [
    "print(\"C Model:\\n\" + \", \".join(C_train.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
